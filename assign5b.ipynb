{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e553b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48590772",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"content/corona.txt\",\"r\")\n",
    "covid_data= [text for text in data if text.count(\"\")>=2]\n",
    "vectorize=Tokenizer()\n",
    "vectorize.fit_on_texts(covid_data)\n",
    "covid_data=vectorize.texts_to_sequences(covid_data)\n",
    "total_vocab=sum(len(s) for s in covid_data)\n",
    "word_count=len(vectorize.word_index)+1\n",
    "window_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1dc7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_model(data,windows_size, total_vocab):\n",
    "  total_length=window_size*2\n",
    "  for text in data:\n",
    "    text_len=len(text)\n",
    "    for idx, word in enumerate(text):\n",
    "      context_word=[]\n",
    "      target=[]\n",
    "      begin=idx-window_size\n",
    "      end=idx+window_size+1\n",
    "      context_word.append([text[i] for i in range(begin,end) if 0<- i< text_len and i!=idx])\n",
    "      target.append(word)\n",
    "      contextual = sequence.pad_sequences(context_word, total_length=total_length)\n",
    "      final_target=np_utils.to_categorical(target, total_vocab)\n",
    "      yield(contextual, final_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2869d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 0\n",
      "9 0\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=total_vocab,output_dim=100,input_length=window_size*2))\n",
    "model.add(Lambda(lambda x:K.mean(x,axis=1), output_shape=(100,)))\n",
    "model.add(Dense(total_vocab, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "for i in range(10):\n",
    "  cost=0\n",
    "  for x, y in cbow_model(data,window_size, total_vocab):\n",
    "    cost+=model.train_on_batch(contextual, final_target)\n",
    "  print(i, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40d38ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions = 100\n",
    "vect_file=open(\"content/vector.txt\", \"w\")\n",
    "vect_file.write('{} {}\\n'.format(total_vocab, dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a883fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=model.get_weights()[0]\n",
    "for text, i in vectorize.word_index.items():\n",
    "  final_vec=\"\".join(map(str, list(weight[i,:])))\n",
    "  vect_file.write('{}{}\\n'.format(text, final_vec))\n",
    "vect_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ac8ebc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (0,) into shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cbow_output\u001b[38;5;241m=\u001b[39m\u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent/vector.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cbow_output\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvirus\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1723\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1678\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1679\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1680\u001b[0m     ):\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m \n\u001b[0;32m   1683\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2073\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2069\u001b[0m         _word2vec_read_binary(\n\u001b[0;32m   2070\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2071\u001b[0m         )\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[43m_word2vec_read_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv):\n\u001b[0;32m   2075\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2076\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2077\u001b[0m         kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(kv),\n\u001b[0;32m   2078\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1979\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1978\u001b[0m word, weights \u001b[38;5;241m=\u001b[39m _word2vec_line_to_vector(line, datatype, unicode_errors, encoding)\n\u001b[1;32m-> 1979\u001b[0m \u001b[43m_add_word_to_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1915\u001b[0m, in \u001b[0;36m_add_word_to_kv\u001b[1;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[0;32m   1913\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate word \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in word2vec file, ignoring all but first\u001b[39m\u001b[38;5;124m\"\u001b[39m, word)\n\u001b[0;32m   1914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1915\u001b[0m word_id \u001b[38;5;241m=\u001b[39m \u001b[43mkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# Most common scenario: no vocab file given. Just make up some bogus counts, in descending order.\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     \u001b[38;5;66;03m# TODO (someday): make this faking optional, include more realistic (Zipf-based) fake numbers.\u001b[39;00m\n\u001b[0;32m   1920\u001b[0m     word_count \u001b[38;5;241m=\u001b[39m vocab_size \u001b[38;5;241m-\u001b[39m word_id\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:563\u001b[0m, in \u001b[0;36mKeyedVectors.add_vector\u001b[1;34m(self, key, vector)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_key[target_index] \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_to_index[key] \u001b[38;5;241m=\u001b[39m target_index\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_index\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m vector\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_index\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (0,) into shape (100,)"
     ]
    }
   ],
   "source": [
    "cbow_output=gensim.models.KeyedVectors.load_word2vec_format(\"content/vector.txt\", binary=False)\n",
    "cbow_output.most_similar(positive=[\"virus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f37ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "93aef3013a15eff4702ce26c902c629a92cf59d10afcc52642e52a5f2f528a8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
